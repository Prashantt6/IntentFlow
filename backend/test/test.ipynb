{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78073773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    with open('../intents.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        add_task = data['add_task']\n",
    "        print(add_task)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File doesnt exists\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Not a json file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec193a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de250fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "for phrase in add_task:\n",
    "    for i in sent_tokenize(phrase):\n",
    "        # print(i\n",
    "        word.append(i)\n",
    "        # temp = []\n",
    "        # for j in word_tokenize(i):\n",
    "        #     temp.append(j.lower())\n",
    "        # word.append(temp)\n",
    "# for w in word:\n",
    "#     print(w)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbe428",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(sentences=word, min_count=1,vector_size=10, window=5)\n",
    "print(model1.wv.index_to_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.wv['add task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"yo task gar\"\n",
    "test= []\n",
    "for j in word_tokenize(input):\n",
    "    test.append(j)\n",
    "\n",
    "test_model = gensim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Word embeddings are key for NLP tasks\",\n",
    "    \"I enjoy exploring AI concepts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3286eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.utils import simple_preprocess\n",
    "\n",
    "# processed_sentences = [simple_preprocess(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') #Downloading model\n",
    "\n",
    "embeddings = model.encode(add_task)    \n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'yo task garnu xa'\n",
    "input_embedding = model.encode([input])\n",
    "\n",
    "similarities = cosine_similarity(input_embedding, embeddings)\n",
    "\n",
    "best_match = similarities.argmax()\n",
    "\n",
    "print(\"Best match:\",add_task[best_match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b4a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for real use\n",
    "import json \n",
    "\n",
    "try:\n",
    "    with open('../intents.json','r') as file:\n",
    "        intents = json.load(file)\n",
    "        add_task = intents['add_task']\n",
    "        delete_task = intents['delete_task']\n",
    "        list_task = intents['list_tasks']\n",
    "        write_blog = intents['write_blog']\n",
    "        upload_photo = intents['upload_photo']\n",
    "        delete_photo = intents['delete_photo']\n",
    "except FileNotFoundError:\n",
    "    print(\"File doesnt exixts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca07c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "add_task_embeddings = model.encode(add_task)\n",
    "delete_task_embeddings = model.encode(delete_task)\n",
    "list_task_embeddings = model.encode(list_task)\n",
    "write_blog_embeddings = model.encode(write_blog)\n",
    "upload_photo_embeddings = model.encode(upload_photo)\n",
    "delete_photo_embeddings = model.encode(delete_photo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cf83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_score(input_emb, intent_emb):\n",
    "    similarities = cosine_similarity(input_emb, intent_emb)\n",
    "    return similarities.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ff6415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {'add_task': np.float32(0.64047396), 'delete_task': np.float32(0.58278704), 'list_tasks': np.float32(0.46390554), 'write_blog': np.float32(0.43628132), 'upload_photo': np.float32(0.40013528), 'delete_photo': np.float32(0.44902614)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'add_task'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = \"Yo task add garna malai bhat khanu xa \"\n",
    "input_embeddings = model.encode([test_input])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarities_test():\n",
    "\n",
    "    scores = {\n",
    "       \"add_task\": get_intent_score(input_embeddings, add_task_embeddings),\n",
    "        \"delete_task\": get_intent_score(input_embeddings, delete_task_embeddings),\n",
    "        \"list_tasks\": get_intent_score(input_embeddings, list_task_embeddings),\n",
    "        \"write_blog\": get_intent_score(input_embeddings, write_blog_embeddings),\n",
    "        \"upload_photo\": get_intent_score(input_embeddings, upload_photo_embeddings),\n",
    "        \"delete_photo\": get_intent_score(input_embeddings, delete_photo_embeddings),\n",
    "    }\n",
    "\n",
    "    best_intent = max(scores, key=scores.get)\n",
    "\n",
    "    print(\"Scores:\", scores)\n",
    "    return best_intent\n",
    "\n",
    "similarities_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb0a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
